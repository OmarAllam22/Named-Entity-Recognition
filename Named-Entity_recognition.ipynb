{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc694c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a9d20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('xtreme', name='PAN-X.de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f82c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e42b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9453b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "# Return a DatasetDict if a key doesn't exist\n",
    "panx_ch = defaultdict(DatasetDict) # when defining the default dict, we must define the default value that it returns ... here it returns DatasetDict object\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # Load monolingual corpus\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    # Shuffle and downsample each split according to spoken proportion\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "            ds[split]\n",
    "            .shuffle(seed=0)\n",
    "            .select(range(int(frac * ds[split].num_rows))))\n",
    "            \n",
    "\"\"\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e73f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = data['train'].features['ner_tags'].feature\n",
    "\n",
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch['ner_tags']]}\n",
    "\n",
    "data = data.map(create_tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b92f6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>als</td>\n",
       "      <td>Teil</td>\n",
       "      <td>der</td>\n",
       "      <td>Savoyer</td>\n",
       "      <td>Voralpen</td>\n",
       "      <td>im</td>\n",
       "      <td>Osten</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1    2        3         4   5      6  7\n",
       "tokens  als  Teil  der  Savoyer  Voralpen  im  Osten  .\n",
       "tags      O     O    O    B-LOC     I-LOC   O      O  O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "de_example = data['train'][0]\n",
    "pd.DataFrame([de_example['tokens'], de_example['ner_tags_str']], index=['tokens','tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0dd42cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>137535</td>\n",
       "      <td>69057</td>\n",
       "      <td>68654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>9778</td>\n",
       "      <td>4968</td>\n",
       "      <td>4961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>9290</td>\n",
       "      <td>4569</td>\n",
       "      <td>4750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>8575</td>\n",
       "      <td>4281</td>\n",
       "      <td>4157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train  validation   test\n",
       "O    137535       69057  68654\n",
       "LOC    9778        4968   4961\n",
       "PER    9290        4569   4750\n",
       "ORG    8575        4281   4157"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check if we have imbalanced tokens\n",
    "\n",
    "from collections import Counter, defaultdict \n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "\n",
    "for split, data_vals in data.items():\n",
    "    for row in data_vals['ner_tags_str']:\n",
    "        for tag in row:\n",
    "            if tag.startswith('B'):\n",
    "                tag_type = tag.split('-')[-1]\n",
    "                split2freqs[split][tag_type]+=1\n",
    "            if tag.startswith('O'):\n",
    "                split2freqs[split]['O']+=1\n",
    "pd.DataFrame(split2freqs)\n",
    "\n",
    "# the named-entities are balanced except for the O type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1171e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing:\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = 'xlm-roberta-base'\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080011ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS]', 'My', 'name', 'is', 'Omar', '[SEP]'],\n",
       " ['<s>', '▁My', '▁name', '▁is', '▁Omar', '</s>'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My name is Omar\"\n",
    "\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()\n",
    "bert_tokens, xlmr_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93585d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model:\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
    "\n",
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # load model body\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        # put the tokenClassifier Head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        # load and initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None,\n",
    "                token_type_ids=None, labels=None, **kwargs):\n",
    "        \n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
    "                                 token_type_ids=token_type_ids, **kwargs)\n",
    "        \n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        return TokenClassifierOutput(loss=loss, logits=logits,\n",
    "                                     hidden_states=outputs.hidden_states,\n",
    "                                     attentions=outputs.attentions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e23d5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"xlm-roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.37.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XLMRobertaConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e3556cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = {idx : name for idx, name in enumerate(tags.names)}\n",
    "tag2index = {name : idx for idx, name in enumerate(tags.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae115a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaConfig {\n",
       "  \"_name_or_path\": \"xlm-roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"XLMRobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"O\",\n",
       "    \"1\": \"B-PER\",\n",
       "    \"2\": \"I-PER\",\n",
       "    \"3\": \"B-ORG\",\n",
       "    \"4\": \"I-ORG\",\n",
       "    \"5\": \"B-LOC\",\n",
       "    \"6\": \"I-LOC\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"B-LOC\": 5,\n",
       "    \"B-ORG\": 3,\n",
       "    \"B-PER\": 1,\n",
       "    \"I-LOC\": 6,\n",
       "    \"I-ORG\": 4,\n",
       "    \"I-PER\": 2,\n",
       "    \"O\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"xlm-roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.37.2\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 250002\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the configuration of the xlmr model to pass it to our custom model\n",
    "\n",
    "from transformers import AutoConfig\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,\n",
    "                                         num_labels= tags.num_classes,\n",
    "                                         id2label = index2tag,\n",
    "                                         label2id = tag2index)\n",
    "xlmr_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdec7fd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "custom_xlmr_model = (XLMRobertaForTokenClassification\n",
    "                    .from_pretrained(xlmr_model_name,config = xlmr_config)\n",
    "                    .to(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83096ab1",
   "metadata": {},
   "source": [
    "4. `config` (passed to `XLMRobertaForTokenClassification.from_pretrained`):\n",
    "\n",
    "This argument is optional. By default, `.from_pretrained` will automatically load the configuration of the pre-trained model based on the `xlmr_model_name`. However, in this case, you've created a custom configuration (`xlmr_config`) using `AutoConfig.from_pretrained` and explicitly pass it here. This allows you to override some default parameters defined in the pre-trained model's configuration with your custom values like `num_labels`, `id2label`, and `label2id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5789e163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁My</td>\n",
       "      <td>▁name</td>\n",
       "      <td>▁is</td>\n",
       "      <td>▁Omar</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>2646</td>\n",
       "      <td>9351</td>\n",
       "      <td>83</td>\n",
       "      <td>112493</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0     1      2    3       4     5\n",
       "Tokens     <s>   ▁My  ▁name  ▁is   ▁Omar  </s>\n",
       "Input IDs    0  2646   9351   83  112493     2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = xlmr_tokenizer.encode(text, return_tensors='pt')\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()\n",
    "\n",
    "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=['Tokens','Input IDs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bde91eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 7]), torch.Size([1, 6]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = custom_xlmr_model(input_ids.to(device)).logits\n",
    "predictions = torch.argmax(outputs, dim=-1)\n",
    "outputs.shape, predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa054863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>▁My</td>\n",
       "      <td>▁name</td>\n",
       "      <td>▁is</td>\n",
       "      <td>▁Omar</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0      1      2      3      4      5\n",
       "Tokens       <s>    ▁My  ▁name    ▁is  ▁Omar   </s>\n",
       "Predicted  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC  B-LOC"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [tags.names[i] for i in predictions[0].cpu().numpy()]\n",
    "pd.DataFrame([xlmr_tokens, preds], index=['Tokens', 'Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab967b",
   "metadata": {},
   "source": [
    "we need to mask some tokens like \"row\" in sparrow ... to make the model focus on the main word rather than its subword.\n",
    "\n",
    "Also focus on Content Words: Special tokens like `<s>` (start of sentence) and `</s>` (end of sentence) don't hold meaning by themselves and shouldn't be considered for entity recognition. Masking them forces the model to focus on the actual content words (\"Einwohnern\" in your example) for identifying named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f06188ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'als Teil der Savoyer Voralpen im Osten .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join(de_example['tokens'])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14262c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['als', 'Teil', 'der', 'Savoyer', 'Voralpen', 'im', 'Osten', '.'],\n",
       " ['<s>',\n",
       "  '▁als',\n",
       "  '▁Teil',\n",
       "  '▁der',\n",
       "  '▁Savo',\n",
       "  'yer',\n",
       "  '▁Vor',\n",
       "  'al',\n",
       "  'pen',\n",
       "  '▁im',\n",
       "  '▁O',\n",
       "  'sten',\n",
       "  '▁',\n",
       "  '.',\n",
       "  '</s>'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_example['tokens'],\\\n",
    "xlmr_tokenizer(text).tokens()\n",
    "\n",
    "# as we illustrated above, we need to mask the following:\n",
    "# \"<s>\", \"yer in savoyer\" , \"al, pen in Voralpen\" ... etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f346046c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 4, 4, 4, 5, 6, 6, 7, 7, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs = xlmr_tokenizer(de_example['tokens'], truncation=True,\n",
    "               is_split_into_words=True)\n",
    "tokenized_inputs.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c096d",
   "metadata": {},
   "source": [
    "##### Why use is_split_into_words=False? (default is True)\n",
    "\n",
    "This argument is useful when your input (`de_example['tokens']`) is already pre-processed or tokenized in a specific way that you want to preserve. For example:\n",
    "\n",
    ">* If `de_example['tokens']` contains pre-defined character n-grams (sequences of n characters), setting is_split_into_words=False ensures the tokenizer treats the entire n-gram as a single token.\n",
    ">* If `de_example['tokens']` is a sentence where you've already performed sentence-piece tokenization, setting is_split_into_words=False avoids further splitting by the tokenizer.\n",
    "\n",
    "###### In the code you provided,\n",
    "> `truncation=True` instructs the tokenizer (`xlmr_tokenizer`) to shorten the input text if it exceeds a certain length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec08a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True,\n",
    "                                      is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6284dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True,\n",
    "                      remove_columns=['langs', 'ner_tags', 'tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6979f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = encode_dataset(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b801683",
   "metadata": {},
   "source": [
    "### The following is important library called `seqeval` for performance measure:\n",
    "\n",
    "> Note: `seqeval` expects the inputs to be list of lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4711b0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "          [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "\n",
    "print(classification_report(y_pred, y_true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f48c33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data to be list of lists in order to feed it to seqeval later:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    preds_list , labels_list = [], []\n",
    "    \n",
    "    for batch_idx in range(batch_size):\n",
    "        example_preds, example_labels = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "558c1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model:\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 2\n",
    "batch_size = 16\n",
    "logging_steps = len(data[\"train\"]) // batch_size\n",
    "model_name = f\"Custom-{xlmr_model_name}-finetuned-panx-de\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name, log_level=\"error\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size, \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps=1e6, weight_decay=0.01,\n",
    "    logging_steps=logging_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1996a3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fe0bc60348437ba50da4fc206eed19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f199f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions,\n",
    "                                       eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_pred, y_true)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5fc8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datacollator here padd the sequence with smaller length than input_max_token\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d92a7d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\pyt\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 3:03:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.223300</td>\n",
       "      <td>0.145805</td>\n",
       "      <td>0.840516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.124320</td>\n",
       "      <td>0.868920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.16508057250976563, metrics={'train_runtime': 10986.9578, 'train_samples_per_second': 3.641, 'train_steps_per_second': 0.228, 'total_flos': 837894091031712.0, 'train_loss': 0.16508057250976563, 'epoch': 2.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_init():\n",
    "    return (XLMRobertaForTokenClassification\n",
    "           .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
    "           .to(device))\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args,\n",
    "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                  train_dataset=data_encoded['train'],\n",
    "                  eval_dataset=data_encoded['validation'],\n",
    "                  tokenizer=xlmr_tokenizer)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6057821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'langs', 'ner_tags_str'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacb20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9334fea9d386431f9faadd2e3e88e1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3a408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
